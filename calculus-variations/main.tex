\documentclass[11pt]{penrose}

\usepackage{mathsphystools}
\usepackage{thmstyles}

\newcommand{\missing}[1]{{\color{red}#1}}

\title{MATH 354: Calculus of Variations}
\subtitle{Brief lecture notes}
\author{Rashid M. Talha}
\affiliation{School of Natural Sciences, NUST}
\date{\today}
\begin{document}

\maketitle

\textbf{Textbook:} Calculus of Variations, I. M. Gelfand and S. V. Fomin

\section{Euler-Lagrange Equation}
\begin{nthm}
    Consider the functional
    \begin{equation}
        J[y] = \int_{a}^{b} F(x, y, y') \,dx,
    \end{equation}
    defined on the set of functions $y \in C^{1}[a,b]$, with $y(a) = A$ and $y(b) = B$. Then, a necessary condition for $J[y]$ to have an extremal is that $y$ satisfies the Euler-Lagrange equation
    \begin{equation}
        \frac{\pd F}{\pd y} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'}} = 0.
        \label{eq:el-basic}
    \end{equation}
\end{nthm}
\begin{proof}
    Let $y$ be an extremal of $J[y]$. Let $h \in C^1 [a,b]$ be an arbitrary function with $h(a)=h(b)=0$, and consider the variation $y \to y+h$. If we assume this variation to be small then we can use Taylor expansion to obtain
    \begin{align}
        \delta J
        &= \int_{a}^{b} F(x, y+h, y'+h') \,dx - \int_{a}^{b} F(x, y, y') \,dx\\
        &= \int_{a}^{b} h {\pd F}{\pd y}(x, y, y') + h' {\pd F}{\pd y'}(x, y, y') \,dx\\
        &= \int_{a}^{b} h \paren*{ \frac{\pd F}{\pd y}(x, y, y') + \frac{\pd F}{\pd y'}(x, y, y') } \,dx - \bigg. h(x) F(x, y, y') \bigg|_a^b\\
        &= \int_{a}^{b} h \paren*{ \frac{\pd F}{\pd y}(x, y, y') + \frac{d}{dx} \paren*{ \frac{\pd F}{\pd y'}(x, y, y')} } \,dx.
    \end{align}
    Here, the boundary term vanishes due to $h(a)=h(b)=0$.

    Now, requiring $\delta J = 0$ (the condition for $y$ being an extremal of $J[y]$) and using the fundamental theorem of calculus of variations, we get
    \begin{equation}
        \frac{\pd F}{\pd y}(x, y, y') + \frac{d}{dx} \paren*{ \frac{\pd F}{\pd y'}(x, y, y')} = 0
    \end{equation}
    as required.
\end{proof}

This is a second order ODE in $y$
\begin{equation}
    \frac{\pd F}{\pd y} - \frac{\pd^2 F}{\pd x \pd y'} - y' \frac{\pd^2 F}{\pd y \pd y'} - y'' \frac{\pd^2 F}{\pd y' \pd y'} = 0
\end{equation}
We shall later see that if the highest order derivative contained in the integrand is $y^{(k)}$, then the resulting Euler-Lagrange equation is a $2k$ order ODE. Similarly, if the integrand contains multiple independent variables then we obtain a PDE instead.

The integral curves of equation~\eqref{eq:el-basic} are called extremals (or critical points) of $J$.

\section{First Integrals and Special Cases}
For some integrands $F(x,y,y')$ the Euler-Lagrange equation~\eqref{eq:el-basic} can be reduced into a first order ODE, called the first integral. In some other cases, it can be rearranged into a more practically useful form.

\begin{enumerate}
    \item $F = F(x,y')$\\
    Here, $\frac{\pd F}{\pd y} = 0$. As a result, $\frac{d}{dx} \paren*{\frac{\pd F}{\pd y'}} = 0$, if $y$ satisfies the Euler-Lagrange equation. Therefore, $y$ satisfies the Euler-Lagrange equation if and only if
    \begin{equation}
        \frac{\pd F}{\pd y'} = c
        \label{eq:first-integral-y}
    \end{equation}
    for some constant $c \in \R$.

    \item $F = F(y,y')$\\
    It is easy to show that, in this case, $y$ satisfies the Euler-Lagrange equation if and only if
    \begin{equation}
        F - y' \frac{\pd F}{\pd y'} = c,
        \label{eq:first-integral-x}
    \end{equation}
    for some constant $c \in \R$. This is because
    \begin{align}
        \frac{d}{dx} \paren*{ F - y' \frac{\pd F}{\pd y'} }
        &= \frac{\pd F}{\pd x} + y' \frac{\pd F}{\pd y} + y'' \frac{\pd F}{\pd y'} - y'' \frac{\pd F}{\pd y'} - y' \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'}}\\
        &= y' \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'}} }.
    \end{align}
    So, if the Euler-Lagrange equation~\eqref{eq:el-basic} is satisfied then the LHS is zero. Conversely, if the LHS is zero, then the Euler-Lagrange equation~\eqref{eq:el-basic} is satisfied. (The case $y' = 0$, when the LHS vanishes, leads to $F = \text{constant}$, which again implies that the Euler-Lagrange equation is satisfied.)

    \item $F = F(x,y)$\\
    In this case, the Euler-Lagrange equation reduces to $\frac{\pd F}{\pd y} (x,y) = 0$. This is an algebraic equation; not an ODE.
\end{enumerate}

An important class of functionals that is often encountered is
\begin{equation}
    J[y] = \int_{a}^{b} g(x,y) \sqrt{1 + y'^2} dx.
\end{equation}
Using the Euler-Lagrange equation~\eqref{eq:el-basic} we obtain (after several lines of careful differentiation and algebraic manipulations)
\begin{equation}
    g_y - y g_x - \frac{y''}{1 + y'^2} g = 0.
\end{equation}

For example, if $g(x, y) \equiv 1$, then we immediately obtain $y'' = 0$, which leads to $y = ax + b$ for $a, b \in \R$. This is the solution to the length functional variational problem on $\R^2$.

\begin{nex}
\phantom{.}
    \begin{enumerate}[label=(\roman*)]
        \item Find the extremum of $J[y] = \int_{1}^{2} \frac{1}{x} \sqrt{1 + y'^2} \,dx$ such that $y(1) = 0$ and $y(2) = 1$.
        \item Find the extremum of $J[y] = \int_{x_1}^{x_2} 2\pi y \sqrt{1 + y'^2} \,dx$ subject to $y(x_1) = y_1$ and $y(x_2) = y_2$.
        \item Find the extremum of $J[y] = \int_{x_1}^{x_2} (x-y)^2 dx$.
    \end{enumerate}
\end{nex}

\section{Existence and Uniqueness of Solutions to Variational Problems}
The variational problems are usually posed as boundary value problems, where the solution is required to exist over the entire domain. Unlike the initial value ODE problems, there are no general results for the existence of uniqueness of solutions for boundary value problems. Moreover, the existence and uniqueness theorems for IVPs only discuss the solution in a neighbourhood of the initial point, and this might not extend to cover the entire domain.

A certain class of variational (boundary value) problems, however, does have an associated existence and uniqueness theorem.
\begin{nthm}[Bernstein]
    Consider the ODE $y'' = G(x, y, y')$. If the functions $F$, $F_y$ and $F_{y'}$ are continuous at every finite point $(x,y)$ for any finite $y'$, and if a constant $k > 0$ and function
    \begin{equation*}
        \alpha = \alpha(x,y) \geq 0,
        \qquad
        \beta = \beta(x,y) \geq 0
    \end{equation*}
    (which are bounder in every finite region of the plane) can be found such that
    \begin{equation*}
        F_y (x,y,y') > k,
        \qquad
        \abs*{F(x,y,y')} \leq \alpha y'^2 + \beta
    \end{equation*}
    then one and only one integral curve of the stated ODE passes through any two point $(a, A)$ and $(b, B)$ where $a \neq b$.
\end{nthm}

\section{Multiple Independent Variables}
Suppose the functional $J$ takes as its input a function of multiple variables $z = z(x,y)$. Here, $x$ and $y$ are both independent variables, and $J = J[z]$. (For simplicity we restrict to two independent variables. All of the following results can easily be extended to more independent variables.) In that case, we can write
\begin{equation}
    J[z] = \int_{R} F(x, y, z(x,y), z_x (x,y), z_y (x,y)) \,dx\,dy.
\end{equation}
Here, $R$ is some suitable closed region in the plane. The goal now is to find a function $z = z(x,y)$ such that $z \in C^2$ and $z$ satisfies the prescribed boundary conditions on $\pd R$, and that it extremises the functional $J[z]$.

\begin{nlemma}
    If $\alpha(x,y)$ is a fixed function which is continuous in a closed region $R$, and if the integral
    \begin{equation}
        \int_{R} \alpha(x,y) h(x,y) \,dx\,dy
    \end{equation}
    vanishes for every function $h$ that is twice continuously differentiable on $R$ and equals zero in the boundary $\pd R$, then $\alpha(x,y) \equiv 0$.
\end{nlemma}

The corresponding Euler-Lagrange equation is
\begin{equation}
    \frac{\pd F}{\pd z}
    - \frac{\pd}{\pd x} \paren*{\frac{\pd F}{\pd z_x}}
    - \frac{\pd}{\pd y} \paren*{\frac{\pd F}{\pd z_y}}
    = 0
\end{equation}

% STATE THIS AS A THEOREM AND PROVE IT (USES GREEN'S THEOREM)

\begin{negg}
    Consider the functional
    \begin{equation}
        J[z] = \int_{R} \paren*{z_{x}^{2} + z_{y}^{2} + 2zf(x,y)} \,dx\,dy.
    \end{equation}

    Its Euler-Lagrange equation is the Poisson equation
    \begin{equation}
        z_{xx} + z_{yy} = f(x,y)
    \end{equation}
\end{negg}

\begin{negg}
    Finding the surface $z = z(x,y)$ of least area, bounded by a given contour can be represented as a variational problem with the functional
    \begin{equation}
        J[z] = \int_{R} \sqrt{1 + z_{x}^{2} + z_{y}^{2}} \,dx\,dy.
    \end{equation}

    The Euler-Lagrange equation for this functional becomes
    \begin{equation}
        r (1 + q^2) - 2spq + t(1 + p^2) = 0
    \end{equation}
    where $p = z_x$, $q = z_y$, $r = z_{xx}$, $s = z_{xy}$ and $t = z_{yy}$.

    This is a complicated non-linear PDE. However, a result from differential geometry states that for a surface given by $z = z(x,y)$, the mean curvature $M$ takes the form
    \begin{equation}
        M = \frac{r (1 + q^2) - 2spq + t(1 + p^2)}{\sqrt{1 + p^2 + q^2}}
    \end{equation}

    Therefore, the required surfaces are precisely those whose mean curvature vanishes. Surfaces whose mean curvature is zero are called minimal surfaces.
\end{negg}

\section{Case of Variable End Point}
Suppose we need to find the function $y = y(x)$ that extremises the functional
\begin{equation}
    J[y] = \int_{x_1}^{x_2} F(x, y, y') \,dx
\end{equation}
subject to the the condition $y(x_1) = y_1$, but whose second endpoint $y(x_2)$ is not fixed.

In this case the variation $\delta J$ with $y \to y+h$ requires $h(x_1) = 0$ but $h(x_2)$ is not fixed. From the usual procedure of Taylor expansion we obtain
\begin{align}
    \delta J
    &= \int_{x_1}^{x_2} h(x) \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} } \,dx
    \;- h(x_1) \left.\frac{\pd F}{\pd y'}\right|_{x_1}
    \;- h(x_2) \left.\frac{\pd F}{\pd y'}\right|_{x_2}\\
    &= \int_{x_1}^{x_2} h(x) \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} } \,dx
    \;- h(x_2) \left.\frac{\pd F}{\pd y'}\right|_{x_2}
\end{align}

This variation needs to be zero for all suitable $h$. In particular, it must vanish for the case where $h(x_2) = 0$. The second boundary term vanishes for this choice of $h$, and, using the fundamental theorem of calculus of variations, we get
\begin{equation}
    \delta J
    = \int_{x_1}^{x_2} h(x) \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} } \,dx = 0
    \implies
    \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} = 0.
\end{equation}
This shows that if $y$ is a critical point of $J$ then it must satisfy the Euler-Lagrange equation. However, note that this Euler-Lagrange equation is independent of $h$. Therefore, it must remain zero even when a different choice of $h$ is selected. In particular, if we revert to the case that $h$ is arbitrary (subject to $h(x_1) = 0$) then the variation $\delta J$ reduces to
\begin{equation}
    \delta J
    = \int_{x_1}^{x_2} h(x) \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} } \,dx
    \;- h(x_2) \left.\frac{\pd F}{\pd y'}\right|_{x_2}
    = - h(x_2) \left.\frac{\pd F}{\pd y'}\right|_{x_2}.
\end{equation}
Therefore, $\delta J = 0$ gives
\begin{equation}
    \left.\frac{\pd F}{\pd y'}\right|_{x_2} = 0.
\end{equation}

Overall, a function $y \in C^2[x_1, x_2]$ is a critical point of the given functional $J[y]$ subject to the condition $y(x_1) = y_1$ only if it satisfies the \emph{system} of equations
\begin{equation}
    \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} = 0
    \quad\text{and}\quad
    \frac{\pd F}{\pd y'} \paren*{ x_2, y(x_2), y'(x_2) } = 0.
\end{equation}

Similarly, if the other point is fixed (while the first varies) then the condition becomes
\begin{equation}
    \frac{\pd F}{\pd y'} \paren*{ x_1, y(x_1), y'(x_1) } = 0.
\end{equation}

Likewise, if both end points are variable, then the resulting set of equations is
\begin{equation}
    \begin{gathered}
        \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} = 0,\\
        \frac{\pd F}{\pd y'} \paren*{ x_1, y(x_1), y'(x_1) } = 0
        \quad\text{and}\quad
        \frac{\pd F}{\pd y'} \paren*{ x_2, y(x_2), y'(x_2) } = 0.
    \end{gathered}
\end{equation}

\begin{negg}
    Consider a massive particle in the $xy$-plane. It slide down under the influence of gravity. Find the trajectory $y=y(x)$ such that this particle reaches the vertical line $x=b$ in the shortest time under the influence of gravity. Without loss of generality, we take the starting point to be the origin $(0,0)$ and assume that $b > 0$.

    This corresponds to minimising the functional
    \begin{equation}
        T[y] = \int_{a}^{b} \sqrt{\frac{1 + y'^2}{2 g y}} \,dx.
    \end{equation}
    subject to $y(0)=0$ (and $y(b)$ is not fixed).

    This functional is independent of $x$ so the associated first integral gives
    \begin{equation}
        F - y' \frac{\pd F}{\pd y'} = \text{constant}
        \implies
        \frac{1}{ \sqrt{y \paren*{1 + y'^2}} } = k
        \implies
        y'^2 = \frac{k}{y} - 1
    \end{equation}
    where $k$ is an arbitrary constant.

    We solve this by introducing a parameter $\theta$, such that $y' = \cot\paren*{\frac{\theta}{2}}$. Consequently,
    \begin{equation}
        y'^2 = \frac{k}{y} - 1
        \implies
        1 + \cot^2 \frac{\theta}{2} = \frac{k}{y}
        \implies
        y = k \sin^2 \frac{\theta}{2}
        \implies
        y = \frac{k}{2} \paren*{1 - \cos\theta}
    \end{equation}
    and
    \begin{equation}
        \frac{dy}{dx} = \cot\frac{\theta}{2}
        \implies
        dx = \tan\frac{\theta}{2} \,dy
        \implies
        x = k \int \tan\frac{\theta}{2} \sin\frac{\theta}{2} \cos\frac{\theta}{2}\,d\theta.
    \end{equation}
    Here, we used $y = k \sin^2 \frac{\theta}{2}$ to get $dy = k \sin\frac{\theta}{2} \cos\frac{\theta}{2} \,d\theta$. This integral can be evaluated to obtain the corresponding parametric expression  for $x$ by using elementary trigonometric relations
    \begin{equation}
        x
        = k \int \sin^2 \paren*{\frac{\theta}{2}} \,d\theta
        = \frac{k}{2} \int \paren*{1 - \cos\theta} \,d\theta
        = c + \frac{k}{2} \paren*{\theta - \sin\theta}
    \end{equation}
    Overall,
    \begin{equation}
        x = c + \frac{k}{2} \paren*{\theta - \sin\theta},
        \quad
        y = \frac{k}{2} \paren*{1 - \cos\theta}
    \end{equation}

    At one end point we have $y = 0$ when $x = 0$. While the variable end point condition
    \begin{equation}
        \left. \frac{\pd F}{\pd y'} \right|_{x=b} = 0
        \implies
        \left. \frac{y'}{ \sqrt{2g} \sqrt{y \paren*{1 + y'^2}}} \right|_{x=b} = 0
    \end{equation}
    gives $y' = 0$ when $x = b$.

    Now, $y = 0$ when $x = 0$ means that we need to simultaneously solve
    \begin{equation}
        c + \frac{k}{2} \paren*{\theta - \sin\theta} = 0
        \quad\text{and}\quad
        \frac{k}{2} \paren*{1 - \cos\theta} = 0
    \end{equation}
    The second equation gives $\theta = 0$, because $k = 0$ would result in the trivial solution $y \equiv 0$. Using this value of $\theta$ in the first equation gives $c = 0$.

    Next, $y' = 0$ at $x = b$ corresponds to the system
    \begin{equation}
        \frac{k}{2} \paren*{\theta - \sin\theta} = b
        \quad\text{and}\quad
        \cot \frac{\theta}{2} = 0.
    \end{equation}
    Again, the second equation gives $\theta = n\pi$, but we can't take $\theta = 0$ otherwise the first equation will reduce to the contradiction $0 = b > 0$. Taking $\theta = \pi$ gives $k = 2b/\pi$. Therefore, the solution to this variational problem is a cycloid
    \begin{equation}
        x = \frac{b}{\pi} \paren*{\theta - \sin\theta},
        \quad
        y = \frac{b}{\pi} \paren*{1 - \cos\theta}.
    \end{equation}
\end{negg}

\section{Variational Derivative}
Let $f$ be a real-valued function of $n$ variables. We study its differential and partial derivatives in real analysis. Analogous concepts also exist for real-valued functional. Among these, the concept of partial derivatives is generalised to the concept of a variational derivative. The procedure is carried out as follows:

We approximate the continuous function $y = y(x)$ as a polygonal line
\begin{equation}
    (x_0, y_0), \dots, (x_{n+1}, y_{n+1}).
\end{equation}
This discretises the functional $J[y] = \int_a^b F(x, y, y') \,dx$ subject to $y(a) = A$ and $y(b) = B$ into a function of $n$ variables
\begin{equation}
    J(y_1, \dots, y_n)
    = \sum_{i=0}^{n} F \paren*{x_i, y_i, \frac{y_{i+1} - y_i}{\Delta x}} \Delta x.
\end{equation}
(This is not a function of $n+2$ variables because $y_0 = A$ and $y_{n+1} = B$ are fixed.)

We can now calculate $\pd J / \pd y_k$ as the usual partial derivative to obtain
\begin{align*}
    \frac{\pd J}{\pd y_k} (y_1, \dots, y_n)
    = {} & \frac{\pd F}{\pd y} \paren*{x_k, y_k, \frac{y_{k+1} - y_k}{\Delta x}} \Delta x\\
    &+ \frac{\pd F}{\pd y'} \paren*{x_{k-1}, y_{k-1}, \frac{y_k - y_{k-1}}{\Delta x}} \Delta x
    - \frac{\pd F}{\pd y'} \paren*{x_k, y_k, \frac{y_{k+1} - y_k}{\Delta x}} \Delta x.
\end{align*}
Dividing by $\Delta x$ and taking the limit $\Delta x \to 0$ reduces this expression to
\begin{equation}
    \frac{\pd J}{\pd y_k \Delta x}
    = \frac{\pd F}{\pd y} (x, y, y')
    - \frac{d}{dx} \paren*{ \frac{\pd F}{\pd y'} (x, y, y') },
\end{equation}
where
\begin{equation}
    \frac{\delta J}{\delta y} \equiv \lim_{\Delta x \to 0} \frac{\pd J}{\pd y_k \Delta x}
\end{equation}
is called the variational derivative of the functional $J[y]$. We note that the variational derivative is precisely the LHS of the Euler-Lagrange equation.

\section{Invariance of the Euler-Lagrange Equation}
So far we have studied the variation of functional
\begin{equation}
    J[y] = \int_a^b F(x, y, y') \,dx
\end{equation}
using the Cartesian variables $y = y(x)$, leading to the Euler-Lagrange equation
\begin{equation}
    \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} = 0.
\end{equation}

Now we consider a change of variables $x = x(u, v)$, $y = y(u, v)$ with a non-vanishing Jacobian, and the assumption that the required solution curves have the form $v = v(u)$. Simple algebra shows that this transform the function $J$ into another functional
\begin{equation}
    J_1 [v]
    = \int_\alpha^\beta F\paren*{x\paren*{u, v}, y(u,v), \frac{y_u + y_v v'}{x_u + x_v v'}} \paren*{x_u + x_v v'} \,du
    = \int_\alpha^\beta F_1 (u, v, v') \,du
\end{equation}
with
\begin{equation}
    F_1 (u, v, v')
    = F\paren*{x(u, v), y(u,v), \frac{y_u + y_v v'}{x_u + x_v v'}} \paren*{x_u + x_v v'}.
\end{equation}

It can be shown that if $y = y(x)$ satisfies the Euler-Lagrange equation
\begin{equation}
    \frac{\pd F}{\pd y} - \frac{d}{dx}\paren*{\frac{\pd F}{\pd y'}} = 0,
\end{equation}
then $v = v(u)$ satisfies the Euler-Lagrange equation
\begin{equation}
    \frac{\pd F_1}{\pd v} - \frac{d}{du}\paren*{\frac{\pd F_1}{\pd v'}} = 0,
\end{equation}
corresponding to $J_1$. We note that the form of the Euler-Lagrange equation remains unchanged even when we transform to new variables; the form of the functional and the integrand does not (in general) remain the same. In other words, whether or not a curve is an extremal is a property that is independent of the choice of coordinates.

This result is known as the invariance of the Euler-Lagrange equation.

One way to prove this result is through the use of the variational derivative.

\begin{negg}
    Consider the functional
    \begin{equation}
        J[r] = \int_{\theta_0}^{\theta_1} \sqrt{r^2 + r'^2} \,d\theta
    \end{equation}
    where $r = r(\theta)$. Its associated Euler-Lagrange equation is
    \begin{equation}
        \frac{r}{\sqrt{r^2 + r'^2}} - \frac{d}{d\theta}\paren*{ \frac{r'}{\sqrt{r^2 + r'^2}} } = 0.
    \end{equation}
    On the other hand, if we transform to new variables $x = r \cos\theta$ and $y = r \sin\theta$ and seek a solution of the form $y = y(x)$, then the functional becomes
    \begin{equation}
        \int_\alpha^\beta \sqrt{1 + y'^2} \,dx,
    \end{equation}
    leading to the Euler-Lagrange equation $y'' = 0$. This is significantly easier to solve. The general solution is $y = a x + b$. Or in the original coordinates
    \begin{equation}
        r \sin\theta = a r \cos\theta + b.
    \end{equation}
\end{negg}

\section{Generalisation to Many Dependent Variables}
Suppose we are given a functional of the form
\begin{equation}
    J[y_1, \dots, y_n] = \int_a^b F\paren*{x, y_1, \dots, y_n, y'_1, \dots, y'_n} \,dx
\end{equation}
where $y_i \in C^2 [a,b]$ for all $i=1, \dots, n$. Previously we only had a single dependent variable $y = y(x)$ and obtained a single second-order ODE as the Euler-Lagrange equation corresponding to $\delta J / \delta y = 0$. In the current generalisation, this necessary conditions for the extremal will become $\delta J / \delta y_i = 0$ for each $i$, and we will obtain a system of $n$ second-order Euler-Lagrange equations
\begin{equation}
    \frac{\pd F}{\pd y_i} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_i}} = 0,
    \quad i = 1, \dots, n.
\end{equation}

This conclusion follows from varying the functional $J[y_1, \dots, y_n]$ as follows. Consider $n$ independent arbitrary functions $h_i \in C^2[a,b]$ such that $h_i(a) = h_i(b) = 0$ for all $i = 1, \dots, n$. Let $y_i$ denote the extremal of $J$. Then, varying it as $y_i \to y_i + h_i$ gives
\begin{align}
    \delta J
    &= \int_a^b F\paren*{x, y_1 + h_1, \dots, y_n + h_n, y'_1 + h'_1, \dots, y'_n + h'_n} \,dx\\
    &\qquad - \int_a^b F\paren*{x, y_1, \dots, y_n, y'_1, \dots, y'_n} \,dx\\
    &= \int_a^b \sum_{i=1}^{n} \paren*{h_i \frac{\pd F}{\pd y_i} + h'_i \frac{\pd F}{\pd y'_i}} \,dx\\
    &= \bigg. h_i(x) F(x, y_i, y'_i) \bigg|_{a}^{b}
    + \int_a^b \sum_{i=1}^{n} h_i \paren*{ \frac{\pd F}{\pd y_i} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_i}}} \,dx\\
    &= \int_a^b \sum_{i=1}^{n} h_i \paren*{ \frac{\pd F}{\pd y_i} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_i}}} \,dx.
\end{align}

Now, since $h_i$ are arbitrary (except the regularity and boundary conditions), we can consider the case where all $h_i$ are zero except $h_k$. This reduces the variation of $J$ to
\begin{equation}
    \delta J = \int_a^b h_k \paren*{ \frac{\pd F}{\pd y_k} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_k}}} \,dx.
\end{equation}
Requiring $\delta J = 0$ and using the fundamental theorem of calculus of variations then gives
\begin{equation}
    \frac{\pd F}{\pd y_k} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_k}} = 0.
\end{equation}
The same process can be repeated for each $h_i$ to get one Euler-Lagrange equation for each dependent variable.

\section{Geodesics}
Functionals with multiple dependent variables are particularly suitable for stating the problem of finding geodesics on a surface. A geodesic between two points $P$ and $Q$ on a surface $\vec{r} = \vec{r}(u,v)$ is a curve of shortest length connecting these two points. In finding a geodesic we, therefore, minimise the length functional
\begin{equation}
    L = \int_\alpha^\beta ds,
\end{equation}
where $ds$ is the length element for the given surface. From differential geometry, we know that
\begin{equation}
    ds^2 = E(u,v) \,du^2 + 2 F(u,v) \,du dv + G(u,v) \,dv^2,
    \label{eq:1st-fund-form}
\end{equation}
with $E = \vec{r}_u \cdot \vec{r}_u$, $F = \vec{r}_u \cdot \vec{r}_v$ and $G = \vec{r}_v \cdot \vec{r}_v$. Equation \eqref{eq:1st-fund-form} is also called the first fundamental form for the surface $\vec{r} = \vec{r}(u,v)$. Consequently, finding geodesics on the given surface is equivalent to finding the extremals $u = u(t), v = v(t)$ of
\begin{equation}
    J[u, v] = \int_\alpha^\beta \sqrt{E(u,v) u'^2 + 2 F(u,v) u' v' + G(u,v) v'^2} \,dt.
\end{equation}
The resulting Euler-Lagrange equations are
\begin{gather}
    \frac{E_u u'^2 + 2F_u u' v' + G_u v'^2}{2Q} - \frac{d}{dt}\paren*{ \frac{E u' + F v'}{Q} } = 0,\\
    \frac{E_v u'^2 + 2F_v u' v' + G_v v'^2}{2Q} - \frac{d}{dt}\paren*{ \frac{F u' + G v'}{Q} } = 0,
\end{gather}
where $Q = \sqrt{E(u,v) u'^2 + 2 F(u,v) u' v' + G(u,v) v'^2}$.

\begin{negg}
    To find the equation of a general geodesic on a circular cylinder of radius $a$ and centred on the $z$-axis, we start by parametrising the surface as
    \begin{equation}
        \vec{r}(\varphi, z) = \paren*{a\cos\varphi, a\sin\varphi, z}.
    \end{equation}
    Then, $E = \vec{r}_\varphi \cdot \vec{r}_\varphi = a^2$, $F = \vec{r}_\varphi \cdot \vec{r}_z = 0$ and $G = \vec{r}_z \cdot \vec{r}_z = 1$. So, the length functional becomes
    \begin{equation}
        J[\varphi, z] = \int_\alpha^\beta \sqrt{a^2 \dot{\varphi}^2 + \dot{z}^2} \,dt,
    \end{equation}
    leading to a system of two Euler-Lagrange equations (first integrals)
    \begin{equation}
        \frac{d\varphi}{dt} = k_1 \sqrt{a^2 \dot{\varphi}^2 + \dot{z}^2},
        \qquad
        \frac{dz}{dt} = k_2 \sqrt{a^2 \dot{\varphi}^2 + \dot{z}^2}.
    \end{equation}
    Here, $k_1, k_2$ are arbitrary constants. We can solve these by eliminating the parameter $t$ and giving $z$ as a function of $\varphi$,
    \begin{equation}
        \frac{dz/dt}{d\varphi/dt}
        = \frac{k_2 \sqrt{a^2 \dot{\varphi}^2 + \dot{z}^2}}{k_1 \sqrt{a^2 \dot{\varphi}^2 + \dot{z}^2}}
        \implies
        \frac{dz}{d\varphi} = c_1
        \implies
        z = c_1 \varphi + c_2.
    \end{equation}
    The arbitrary constant $c_1$, $c_2$ are determined by the points $P$ and $Q$.
\end{negg}

\begin{negg}
    Suppose that the $3$-dimensional space is filled with an optically inhomogeneous medium, such that the velocity of propagation of light at each point is some function $v(x, y, z)$ of the coordinates of the points.

    According to Fermat's principle, light travels from point $A$ to point $B$ along the trajectory which takes the least time. Suppose the curve joining $A$ and $B$ is given by the equations $y = y(x)$ and $z = z(x)$. Then, the corresponding time functional is
    \begin{equation}
        T[y, z] = \int_a^b \frac{\sqrt{1 + y'^2 + z'^2}}{v(x, y, z)} \,dx.
    \end{equation}
    The associated Euler-Lagrange equations are
    \begin{gather}
        \frac{\sqrt{1 + y'^2 + z'^2}}{v^2} v_y
        + \frac{d}{dx}\paren*{\frac{y'}{\sqrt{1 + y'^2 + z'^2}}} = 0,\\
        \frac{\sqrt{1 + y'^2 + z'^2}}{v^2} v_z
        + \frac{d}{dx}\paren*{\frac{z'}{\sqrt{1 + y'^2 + z'^2}}} = 0.
    \end{gather}
\end{negg}

\section{Equivalent Functionals}
Let $\varphi = \varphi(x, y_1, \dots, y_n)$ be a twice continuously differentiable function and consider the function
\begin{equation}
    \psi(x, y_1, \dots, y_n, y'_1, \dots, y'_n)
    = \frac{d\varphi}{dx}
    \equiv \frac{\pd \varphi}{\pd x} + \sum_{i=1}^{n} y'_i \frac{\pd \varphi}{\pd y_i}.
\end{equation}
It is straightforward to check by direct calculation that
\begin{equation}
    \frac{\pd \psi}{\pd y_k} - \frac{d}{dx} \paren*{\frac{\pd \psi}{\pd y'_k}} \equiv 0
\end{equation}
identically. Consequently, the Euler-Lagrange equations obtained from
\begin{equation}
    J = \int_a^b F\paren*{x, y_1, \dots, y_n, y'_1, \dots, y'_n} \,dx
\end{equation}
and
\begin{equation}
    J_1 = \int_a^b \paren*{F\paren*{x, y_1, \dots, y_n, y'_1, \dots, y'_n} + \psi(x, y_1, \dots, y_n, y'_1, \dots, y'_n)} \,dx
\end{equation}
are the same; namely
\begin{equation}
    \frac{\pd F}{\pd y_k} - \frac{d}{dx} \paren*{\frac{\pd F}{\pd y'_k}} = 0.
\end{equation}
This can also be seen by noting that $J$ and $J_1$ differ only by a constant, since $\psi$ is a total derivative of $\varphi(x, y_1, \dots, y_n)$ and integrates out
\begin{equation}
    J_1 = \int_a^b \paren*{F + \psi} \,dx
    = \int_a^b F \,dx + \int_a^b \frac{d\phi}{dx} \,dx
    = J + \paren*{\big. \varphi \big|_a^b}
    = J + \text{constant}.
\end{equation}

Therefore, different integrands (or likewise, different functionals) can lead to the same Euler-Lagrange equations.

Two functionals are said to be equivalent if they lead to the same Euler-Lagrange equations.

In particular, we note that the functionals $J = \int_a^b F \,dx$ and $J = \int_a^b \paren*{F + \frac{d\phi}{dx}} \,dx$ are equivalent. Similarly, $J = \int_a^b F \,dx$ and $J = \int_a^b cF \,dx$ are equivalent for $c \in \R$.

\section{Functionals Depending on Higher-Order Derivatives}
Now we consider the generalisation to functionals of the form
\begin{equation}
    J[y] = \int_a^b F(x, y, y', \dots, y^{(n)}) \,dx
\end{equation}
with $y(a) = A$, $y(b) = B$ and $y^{(i)}(a) = A_i$, $y^{(i)}(b) = B_i$ for $i=1, \dots, n$.

As usual we suppose that $y$ extremises this functional and consider its variation $y \to y + h$ where $h(a) = h(b) = 0$ and $h^{(i)}(a) = h^{(i)}(b) = 0$ for $i=1, \dots, n$. Then, using Taylor expansion we obtain
\begin{align}
    \delta J
    &= \int_a^b F\paren*{x, y+h, y'+h', \dots, y^{(n)}+h^{(n)}} \,dx
    - \int_a^b F\paren*{x, y, y', \dots, y^{(n)}} \,dx\\
    &= \int_a^b \paren*{h \frac{\pd F}{\pd y} + h' \frac{\pd F}{\pd y'} + \dots + h^{(n)} \frac{\pd F}{\pd y^{(n)}}} \,dx\\
    &= \int_a^b h(x) \paren*{ \frac{\pd F}{\pd y} - \frac{d}{dx} \frac{\pd F}{\pd y'} + \frac{d^2}{dx^2} \frac{\pd F}{\pd y''} + \dots + (-1)^n \frac{d^n}{dx^n} \frac{\pd F}{\pd y^{(n)}} } \,dx.
\end{align}
Here, we have used integration by parts repeatedly in the last step and the boundary terms vanish due to the conditions on $h$ and its derivatives. This last expression is true for all arbitrary functions $h$ (subject to suitable boundary conditions), therefore, the fundamental theorem of calculus of variations gives
\begin{equation}
    \frac{\pd F}{\pd y} - \frac{d}{dx} \frac{\pd F}{\pd y'} + \frac{d^2}{dx^2} \frac{\pd F}{\pd y''} + \dots + (-1)^n \frac{d^n}{dx^n} \frac{\pd F}{\pd y^{(n)}} = 0
\end{equation}
when $\delta J = 0$. In other words, we have proved the following theorem.

\begin{nthm}
    A necessary condition for $y \in C^{2n}[a,b]$ to be an extremal of
    \begin{equation}
        J[y] = \int_a^b F\paren*{x, y, y^{(1)}, \dots, y^{(n)}} \,dx
    \end{equation}
    subject to $y(a) = A$, $y(b) = B$, $y^{(i)}(a) = A_i$ and $y^{(i)}(b) = B_i$, for $i=1, \dots, n$, is given by the (generalised) Euler-Lagrange equation
    \begin{equation}
        \frac{\pd F}{\pd y} + \sum_{k=1}^{n} (-1)^k \frac{d^k}{dx^k} \paren*{\frac{\pd F}{\pd y^{(k)}}} = 0.
        \label{eq:el-higher-deriv}
    \end{equation}
\end{nthm}

Equation~\eqref{eq:el-higher-deriv} is in general an ODE of order $2n$. The regularity conditions on $y$ can be reduced to $y \in C^n$ because it can be shown that under suitable conditions such a function is $y \in C^{(2n)}$.

% \section{Variational Problems in Parametric Form}

\end{document}